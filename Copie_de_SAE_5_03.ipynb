{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "042Rcm6KcXSk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_json(\"train.jsonl\", lines=True) #on importe le fichier .jsonl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ju1YV8hFQX",
        "outputId": "7371e651-cbff-414f-da75-008723a66389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9g-YX5Cmtrf",
        "outputId": "2663e89a-4642-476c-d68e-22cd51141333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### On procède au prétraitement\n",
        "stop_words = set(stopwords.words('french'))\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# On charge les données\n",
        "data = pd.read_json(\"train.jsonl\", lines=True)\n",
        "\n",
        "# Étape 1 : Nettoyage du texte\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Mise en minuscules\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # caractères spéciaux et des chiffres supprimés\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # espaces en trop supprimés\n",
        "    return text\n",
        "\n",
        "# Étape 2 : Lemmatisation\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_words = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Appliquer le nettoyage et la lemmatisation\n",
        "data['cleaned_text'] = data['texte_annonce'].apply(clean_text).apply(lemmatize_text)\n",
        "\n",
        "# Étape 3 : Vectorisation avec TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.8)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Étape 4 : Réduction de dimension\n",
        "# Truncated SVD\n",
        "svd = TruncatedSVD(n_components=100)\n",
        "X_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "\n",
        "# Étape 5 : Création de variables additionnelles\n",
        "data['text_length'] = data['cleaned_text'].apply(lambda x: len(x.split()))  # Longueur du texte\n",
        "data['contains_url'] = data['texte_annonce'].apply(lambda x: 1 if 'http' in x else 0)  # Présence d'URL\n",
        "data['contains_numbers'] = data['texte_annonce'].apply(lambda x: 1 if re.search(r'\\d', x) else 0)  # Présence de chiffres\n",
        "\n"
      ],
      "metadata": {
        "id": "dvOQv3xKdIhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporter le DataFrame prétraité en CSV\n",
        "# Adding the escapechar parameter to handle special characters\n",
        "data.to_csv(\"data_preprocessed.csv\", index=False, escapechar='\\\\')\n",
        "\n",
        "# Ou, pour un fichier binaire plus rapide à charger\n",
        "data.to_pickle(\"data_preprocessed.pkl\")"
      ],
      "metadata": {
        "id": "Td4FobOvqaLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher le DataFrame transformé\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk0JXcPugekw",
        "outputId": "eeeeb298-abb8-4280-b244-3e254818c1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   OGC_FID boamp_id_annonce boamp_parent_annonce  \\\n",
            "0     9723        23-177335                 None   \n",
            "1    10502         24-63002                 None   \n",
            "2    10900         24-94957         ['24-79402']   \n",
            "3      779        23-179664                 None   \n",
            "4     5614         24-69326                 None   \n",
            "\n",
            "                                   boamp_theme_boamp boamp_libelle_annonce  \\\n",
            "0                                      Espaces verts        Avis de marché   \n",
            "1                      Station d'épuration (travaux)        Avis de marché   \n",
            "2                           Voirie et réseaux divers          Rectificatif   \n",
            "3  Prestations de services, Délégation de service...        Avis de marché   \n",
            "4                                        Génie civil        Avis de marché   \n",
            "\n",
            "  boamp_statut_annonce boamp_date_fin_de_marche  \\\n",
            "0              INITIAL               2024-02-02   \n",
            "1              INITIAL               2024-08-08   \n",
            "2         RECTIFICATIF               2024-08-23   \n",
            "3              INITIAL               2024-04-16   \n",
            "4              INITIAL               2024-07-11   \n",
            "\n",
            "  boamp_num_departement_diffusion  \\\n",
            "0                              31   \n",
            "1                              09   \n",
            "2                              33   \n",
            "3                              75   \n",
            "4                              43   \n",
            "\n",
            "                                  boamp_nom_acheteur boamp_siret_acheteur  \\\n",
            "0                                 TOULOUSE METROPOLE       24310051800170   \n",
            "1  Syndicat Mixte Départemental de l'Eau et de l'...       25090187300019   \n",
            "2                                 Bordeaux Métropole       24330031600011   \n",
            "3                                     Ville de Paris                        \n",
            "4                      Département de la Haute-Loire       22430001200016   \n",
            "\n",
            "   ... cal_num_departement_exec cal_siren_epci cal_insee_commune_exec  \\\n",
            "0  ...                       31      243100518                  31555   \n",
            "1  ...                        9                                 09332   \n",
            "2  ...                       33      243300316                  33063   \n",
            "3  ...                       75                                 75056   \n",
            "4  ...                       43                                 43020   \n",
            "\n",
            "  cal_nom_commune_exec   cal_adresse_annonce  \\\n",
            "0             TOULOUSE  DÉCHÈTERIE DU RAMIER   \n",
            "1            VERNIOLLE                         \n",
            "2             BORDEAUX    ÉGLISE SAINT LOUIS   \n",
            "3                PARIS                         \n",
            "4        BAS EN BASSET                BASSET   \n",
            "\n",
            "                                       texte_annonce  \\\n",
            "0   Avis n° 23-177335\\n \\nAttention : les informa...   \n",
            "1   1/3\\nAvis de marché\\nAttention : les informat...   \n",
            "2   1/1\\nAvis rectificatif\\nAttention : les infor...   \n",
            "3   Avis n° 23-179664\\n \\nAttention : les informa...   \n",
            "4   1/2\\nAvis de marché\\nAttention : les informat...   \n",
            "\n",
            "                                        cleaned_text text_length contains_url  \\\n",
            "0  avis attention information contenu lextrer pdf...        1982            1   \n",
            "1  avis march attention information contenu lextr...         612            1   \n",
            "2  avis rectificatif attention information conten...         132            1   \n",
            "3  avis attention information contenu lextrer pdf...        1303            1   \n",
            "4  avis march attention information contenu lextr...         362            1   \n",
            "\n",
            "  contains_numbers  \n",
            "0                1  \n",
            "1                1  \n",
            "2                1  \n",
            "3                1  \n",
            "4                1  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Charger le fichier .pkl\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "\n",
        "\n",
        "# Définir la variable cible\n",
        "y = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Séparer les données en ensemble d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialiser le modèle de K-NN\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Définir les paramètres à tester\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],   # Nombre de voisins\n",
        "    'weights': ['uniform', 'distance'],  # Pondération des voisins\n",
        "    'p': [1, 2]  # Distance de Minkowski : 1 = Manhattan, 2 = Euclidienne\n",
        "}\n",
        "\n",
        "# Configurer la recherche par grille\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Exécuter la recherche par grille\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs paramètres et score\n",
        "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
        "print(\"Meilleur score de validation:\", grid_search.best_score_)\n",
        "\n",
        "# Entraîner le modèle K-NN optimisé\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred = best_knn.predict(X_test)\n",
        "\n",
        "# Évaluer les performances\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec K-NN optimisé:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-2-UYJ2vHNX",
        "outputId": "0de7bd98-1421-4a8d-9755-73e382ac532d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [    nan 0.6385  0.6385  0.639       nan 0.7275  0.67775 0.67725     nan\n",
            " 0.64    0.69625 0.69625     nan 0.7275  0.70775 0.70775     nan 0.72725\n",
            " 0.71025 0.71025]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleurs paramètres: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
            "Meilleur score de validation: 0.7275\n",
            "Accuracy avec K-NN optimisé: 0.741\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.74      1.00      0.85       741\n",
            "Rejete (hors specs)       0.00      0.00      0.00       259\n",
            "\n",
            "           accuracy                           0.74      1000\n",
            "          macro avg       0.37      0.50      0.43      1000\n",
            "       weighted avg       0.55      0.74      0.63      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commentaires :\n",
        "\n",
        "Nous avons commencé par appliquer le modèle des k plus proches voisins. En optimisant ce modèle avec les meilleurs paramètres, on obtient un meilleur modèle avec 5 plus proches voisins, avec la distance de Manhattan et en donnant plus de poids aux plus proches voisins. Cette optimisation permet nous permet d'aboutir à un accuracy score de 0,74. Le modèle détecte très bien les \"pris en compte\" (taux de précision à 74% Finalement, le modèle des k proches voisins"
      ],
      "metadata": {
        "id": "jUcpa1Ex-HQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assurons-nous que 'X_tfidf' est bien défini\n",
        "# Exemple (décommenter si nécessaire) :\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "# X_tfidf = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# Définir la variable cible\n",
        "y = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Séparer les données en ensemble d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialiser le modèle de régression logistique\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Définir les paramètres à tester\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],               # Paramètre de régularisation\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],      # Type de régularisation\n",
        "    'solver': ['saga'],                         # Saga fonctionne avec l1, l2 et elasticnet\n",
        "    'l1_ratio': [0, 0.5, 1]                     # Ratio pour elasticnet, 0 pour l2, 1 pour l1\n",
        "}\n",
        "\n",
        "# Configurer la recherche par grille\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Exécuter la recherche par grille\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs paramètres et score\n",
        "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
        "print(\"Meilleur score de validation:\", grid_search.best_score_)\n",
        "\n",
        "# Entraîner le modèle de régression logistique optimisé\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "y_pred = best_log_reg.predict(X_test)\n",
        "\n",
        "# Évaluer les performances\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Régression Logistique optimisée:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKq1RyS6wdJB",
        "outputId": "9f4e0d34-7c97-435b-8afd-bf6e1570d05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleurs paramètres: {'C': 10, 'l1_ratio': 0, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Meilleur score de validation: 0.745\n",
            "Accuracy avec Régression Logistique optimisée: 0.773\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.79      0.94      0.86       741\n",
            "Rejete (hors specs)       0.64      0.29      0.39       259\n",
            "\n",
            "           accuracy                           0.77      1000\n",
            "          macro avg       0.71      0.61      0.63      1000\n",
            "       weighted avg       0.75      0.77      0.74      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser le modèle de régression logistique avec les meilleurs paramètres\n",
        "best_log_reg = LogisticRegression(C=10, l1_ratio=0, penalty='l2', solver='saga', max_iter=1000)\n",
        "\n",
        "# Entraîner le modèle avec les données d'entraînement\n",
        "best_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur les données de test\n",
        "y_pred = best_log_reg.predict(X_test)\n",
        "\n",
        "# Évaluer les performances du modèle\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Régression Logistique optimisée:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gynMt9iuswC5",
        "outputId": "f2562cc9-f825-4a81-ef3b-985afce6985a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy avec Régression Logistique optimisée: 0.77\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.77      0.98      0.86       741\n",
            "Rejete (hors specs)       0.74      0.17      0.28       259\n",
            "\n",
            "           accuracy                           0.77      1000\n",
            "          macro avg       0.75      0.58      0.57      1000\n",
            "       weighted avg       0.76      0.77      0.71      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cal_réponse_signalement est notre variable cible\n",
        "y = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Séparer les données en ensemble d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialiser le modèle d'arbre de décision\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Définir les paramètres à tester\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Configurer et exécuter la recherche par grille\n",
        "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs paramètres et score\n",
        "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
        "print(\"Meilleur score de validation:\", grid_search.best_score_)\n",
        "\n",
        "# Entraîner le modèle optimisé avec les meilleurs paramètres\n",
        "best_tree = grid_search.best_estimator_\n",
        "y_pred = best_tree.predict(X_test)\n",
        "\n",
        "# Évaluer les performances\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Arbre de Décision optimisé:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd9CJxb__JQA",
        "outputId": "21a5fcb8-122d-45e3-9f75-8a896e117e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleurs paramètres: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
            "Meilleur score de validation: 0.7220000000000001\n",
            "Accuracy avec Arbre de Décision optimisé: 0.76\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.77      0.96      0.86       741\n",
            "Rejete (hors specs)       0.61      0.20      0.30       259\n",
            "\n",
            "           accuracy                           0.76      1000\n",
            "          macro avg       0.69      0.58      0.58      1000\n",
            "       weighted avg       0.73      0.76      0.71      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Meilleurs paramètres: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Initialiser le modèle d'arbre de décision avec les meilleurs paramètres\n",
        "best_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=4, min_samples_split=10, random_state=42)\n",
        "\n",
        "# Entraîner le modèle avec les données d'entraînement\n",
        "best_tree.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur les données de test\n",
        "y_pred = best_tree.predict(X_test)\n",
        "\n",
        "# Évaluer les performances du modèle\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Arbre de Décision optimisé (avec meilleurs paramètres):\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46sem4mWtDTU",
        "outputId": "834f7abd-74ef-432b-a298-a863c93080bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy avec Arbre de Décision optimisé (avec meilleurs paramètres): 0.749\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.77      0.94      0.85       741\n",
            "Rejete (hors specs)       0.54      0.21      0.30       259\n",
            "\n",
            "           accuracy                           0.75      1000\n",
            "          macro avg       0.66      0.57      0.58      1000\n",
            "       weighted avg       0.71      0.75      0.71      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Définir la variable cible\n",
        "y = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Séparer les données en ensemble d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialiser et entraîner la forêt aléatoire\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)  # n_estimators est le nombre d'arbres\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les labels sur l'ensemble de test\n",
        "y_pred = forest.predict(X_test)\n",
        "\n",
        "# Évaluer les performances\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Forêt Aléatoire:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ADnxo3LADUX",
        "outputId": "3d45aa9f-c3a4-4b2e-b453-9698bf1ee334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy avec Forêt Aléatoire: 0.743\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.75      0.99      0.85       741\n",
            "Rejete (hors specs)       0.57      0.03      0.06       259\n",
            "\n",
            "           accuracy                           0.74      1000\n",
            "          macro avg       0.66      0.51      0.45      1000\n",
            "       weighted avg       0.70      0.74      0.65      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Charger le fichier .pkl\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "\n",
        "\n",
        "# Définir la variable cible\n",
        "y = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Séparer les données en ensemble d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialiser le modèle de forêt aléatoire\n",
        "forest = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Définir les paramètres à tester pour l'optimisation\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],           # Nombre d'arbres dans la forêt\n",
        "    'max_depth': [10, 20, None],              # Profondeur maximale de chaque arbre\n",
        "    'min_samples_split': [2, 5, 10],          # Nombre minimum d'échantillons pour diviser un nœud\n",
        "    'min_samples_leaf': [1, 2, 4]             # Nombre minimum d'échantillons par feuille\n",
        "}\n",
        "\n",
        "# Configurer et exécuter la recherche par grille\n",
        "grid_search = GridSearchCV(forest, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs paramètres et score\n",
        "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
        "print(\"Meilleur score de validation:\", grid_search.best_score_)\n",
        "\n",
        "# Entraîner le modèle optimisé avec les meilleurs paramètres\n",
        "best_forest = grid_search.best_estimator_\n",
        "y_pred = best_forest.predict(X_test)\n",
        "\n",
        "# Évaluer les performances\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Forêt Aléatoire optimisée:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "S-HITouwARv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8273d50-20ed-4f34-8303-74a439a360fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleurs paramètres: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Meilleur score de validation: 0.728\n",
            "Accuracy avec Forêt Aléatoire optimisée: 0.745\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.75      0.99      0.85       741\n",
            "Rejete (hors specs)       0.62      0.04      0.07       259\n",
            "\n",
            "           accuracy                           0.74      1000\n",
            "          macro avg       0.69      0.52      0.46      1000\n",
            "       weighted avg       0.72      0.74      0.65      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Meilleurs paramètres: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
        "\n",
        "# Initialiser le modèle de forêt aléatoire avec les meilleurs paramètres\n",
        "best_forest = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50, random_state=42)\n",
        "\n",
        "# Entraîner le modèle avec les données d'entraînement\n",
        "best_forest.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur les données de test\n",
        "y_pred = best_forest.predict(X_test)\n",
        "\n",
        "# Évaluer les performances du modèle\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy avec Forêt Aléatoire optimisée (avec meilleurs paramètres):\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEPo0ejjtWzs",
        "outputId": "f218e84b-1b1d-481b-d00b-54209d8b7a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy avec Forêt Aléatoire optimisée (avec meilleurs paramètres): 0.748\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.75      1.00      0.85       741\n",
            "Rejete (hors specs)       0.82      0.03      0.07       259\n",
            "\n",
            "           accuracy                           0.75      1000\n",
            "          macro avg       0.78      0.52      0.46      1000\n",
            "       weighted avg       0.77      0.75      0.65      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données prétraitées d'entraînement\n",
        "data_preprocessed = pd.read_pickle(\"/content/data_preprocessed.pkl\")\n",
        "\n",
        "# Définir la variable cible\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Créer la matrice TF-IDF pour les données d'entraînement\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# Initialiser et entraîner le modèle de régression logistique avec les paramètres optimisés\n",
        "log_reg = LogisticRegression(C=10, penalty='l2', solver='saga', max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Charger et prétraiter les données de test\n",
        "test_data = pd.read_json(\"/content/test.jsonl\", lines=True)\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].str.lower().str.replace(r'[^a-z\\s]', '', regex=True)\n",
        "\n",
        "# Transformer les données de test en utilisant le même vectoriseur TF-IDF\n",
        "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# Effectuer les prédictions sur les données de test\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Ajouter les prédictions au DataFrame de test pour référence\n",
        "test_data['predictions'] = y_pred\n",
        "\n",
        "# Exporter les prédictions pour les données de test dans un fichier CSV\n",
        "test_data[['texte_annonce', 'predictions']].to_csv(\"test_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Les prédictions ont été enregistrées dans le fichier 'test_predictions.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkMsciBKFT_u",
        "outputId": "f8e506ba-65fd-476f-96fb-a36131fb010f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les prédictions ont été enregistrées dans le fichier 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Charger les données d'entraînement prétraitées\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Vectoriser les données d'entraînement\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# Charger et prétraiter les données de test\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].str.lower().str.replace(r'[^a-z\\s]', '', regex=True)\n",
        "\n",
        "# Transformer les données de test avec le même vectoriseur TF-IDF\n",
        "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# Initialiser et entraîner le modèle de régression logistique\n",
        "log_reg = LogisticRegression(C=10, penalty='l2', solver='saga', max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les valeurs pour cal_réponse_signalement\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Ajouter les prédictions dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement'] = y_pred\n",
        "\n",
        "# Créer le fichier final avec uniquement les colonnes OCG_FID et cal_réponse_signalement\n",
        "final_output = test_data[['OGC_FID', 'cal_réponse_signalement']]\n",
        "\n",
        "# Exporter le fichier final en CSV\n",
        "final_output.to_csv(\"final_test_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Le fichier 'final_test_predictions.csv' a été généré avec succès.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kufwuw8KHCuk",
        "outputId": "d05c8386-ba40-4107-93d5-07665f3ba3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier 'final_test_predictions.csv' a été généré avec succès.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Charger les données d'entraînement prétraitées\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Vectoriser les données d'entraînement\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# Charger et prétraiter les données de test\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].str.lower().str.replace(r'[^a-z\\s]', '', regex=True)\n",
        "\n",
        "# Transformer les données de test avec le même vectoriseur TF-IDF\n",
        "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# Initialiser le modèle de forêt aléatoire avec les meilleurs paramètres\n",
        "best_forest = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50, random_state=42)\n",
        "\n",
        "# Entraîner le modèle avec les données d'entraînement\n",
        "best_forest.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les valeurs pour cal_réponse_signalement\n",
        "y_pred = best_forest.predict(X_test)\n",
        "\n",
        "# Ajouter les prédictions dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement'] = y_pred\n",
        "\n",
        "# Créer le fichier final avec uniquement les colonnes OGC_FID et cal_réponse_signalement\n",
        "final_output = test_data[['OGC_FID', 'cal_réponse_signalement']]\n",
        "\n",
        "# Exporter le fichier final en CSV\n",
        "final_output.to_csv(\"final_test_predictions_forest.csv\", index=False)\n",
        "\n",
        "print(\"Le fichier 'final_test_predictions_forest.csv' a été généré avec succès.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg3Bw8TxEbFu",
        "outputId": "c683f3e3-b663-46ff-8250-322b844957c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier 'final_test_predictions_forest.csv' a été généré avec succès.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# --- Étape 1 : Charger les données d'entraînement prétraitées ---\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Vectoriser les textes d'entraînement\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "X_train = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# --- Étape 2 : Charger les données de test ---\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "\n",
        "# Nettoyer les textes de test\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Supprimer les caractères spéciaux et chiffres\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].apply(clean_text)\n",
        "\n",
        "# Transformer les textes de test avec le même TF-IDF\n",
        "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# --- Étape 3 : Initialiser le modèle SVM ---\n",
        "# Définir un SVM avec recherche d'hyperparamètres\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Recherche des meilleurs hyperparamètres\n",
        "grid_search = GridSearchCV(svm, param_grid, scoring='f1_weighted', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs paramètres\n",
        "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# --- Étape 4 : Prédictions sur les données de test ---\n",
        "y_test_pred = best_svm.predict(X_test)\n",
        "y_test_probs = best_svm.predict_proba(X_test)[:, 1]  # Probabilités de la classe positive\n",
        "\n",
        "# Ajouter les prédictions dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement_prédite'] = y_test_pred\n",
        "test_data['confidence'] = y_test_probs\n",
        "\n",
        "# --- Étape 5 : Exporter les prédictions ---\n",
        "test_data[['OGC_FID', 'cal_réponse_signalement_prédite', 'confidence']].to_csv(\"svm_test_predictions.csv\", index=False)\n",
        "print(\"Les prédictions du modèle SVM ont été exportées dans 'svm_test_predictions.csv'.\")\n",
        "\n",
        "# --- Étape 6 : Évaluer le modèle sur des données avec des étiquettes (si disponibles) ---\n",
        "# Supposons que vous avez les vraies étiquettes dans une colonne 'cal_réponse_signalement'\n",
        "if 'cal_réponse_signalement' in test_data.columns:\n",
        "    print(\"\\nÉvaluation des performances du SVM :\")\n",
        "    print(classification_report(test_data['cal_réponse_signalement'], y_test_pred))\n",
        "    print(f\"F1-Score : {f1_score(test_data['cal_réponse_signalement'], y_test_pred, pos_label='Pris en compte', average='weighted'):.3f}\")\n"
      ],
      "metadata": {
        "id": "axki6YFuFdgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import re  # Ajouter cette ligne au début de votre code\n",
        "\n",
        "\n",
        "# --- Étape 1 : Charger les données d'entraînement prétraitées ---\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Vectoriser les textes d'entraînement\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
        "X_train = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# --- Étape 2 : Charger les données de test ---\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "\n",
        "# Nettoyer les textes de test\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Supprimer les caractères spéciaux et chiffres\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].apply(clean_text)\n",
        "\n",
        "# Transformer les textes de test avec le même TF-IDF\n",
        "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# --- Étape 3 : Initialiser et entraîner le modèle SVM ---\n",
        "# Utilisation de paramètres fixes pour un essai rapide\n",
        "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# --- Étape 4 : Faire des prédictions sur les données de test ---\n",
        "y_test_pred = svm.predict(X_test)\n",
        "y_test_probs = svm.predict_proba(X_test)[:, 1]  # Probabilités de la classe positive\n",
        "\n",
        "# Ajouter les prédictions dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement_prédite'] = y_test_pred\n",
        "test_data['confidence'] = y_test_probs\n",
        "\n",
        "# --- Étape 5 : Exporter les prédictions ---\n",
        "test_data[['OGC_FID', 'cal_réponse_signalement_prédite']].to_csv(\"svm_test_predictions.csv\", index=False)\n",
        "print(\"Les prédictions du modèle SVM ont été exportées dans 'svm_test_predictions.csv'.\")\n",
        "\n",
        "# --- Étape 6 : Évaluer le modèle sur des données avec des étiquettes (si disponibles) ---\n",
        "# Supposons que vous avez les vraies étiquettes dans une colonne 'cal_réponse_signalement'\n",
        "if 'cal_réponse_signalement' in test_data.columns:\n",
        "    print(\"\\nÉvaluation des performances du SVM :\")\n",
        "    print(classification_report(test_data['cal_réponse_signalement'], y_test_pred))\n",
        "    print(f\"F1-Score : {f1_score(test_data['cal_réponse_signalement'], y_test_pred, pos_label='Pris en compte', average='weighted'):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J2wRlYpVmx9",
        "outputId": "f9ec2879-afb5-48f9-de29-9a8549b0fbb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les prédictions du modèle SVM ont été exportées dans 'svm_test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Charger les données d'entraînement\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Pipeline pour inclure la vectorisation TF-IDF et le modèle SVM\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('svm', SVC(probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "# Définir les grilles de paramètres à tester\n",
        "param_grid = {\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Uni-gramme, bi-gramme\n",
        "    'tfidf__min_df': [1, 5, 10],             # Fréquence min des termes\n",
        "    'tfidf__max_df': [0.5, 0.8, 1.0],        # Fréquence max des termes\n",
        "    'svm__C': [0.1, 1, 10],                  # Régularisation\n",
        "    'svm__kernel': ['linear', 'rbf'],        # Type de noyau\n",
        "    'svm__gamma': ['scale', 'auto']          # Coefficient du noyau\n",
        "}\n",
        "\n",
        "# Initialiser GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='f1_weighted',  # Utiliser le F1-Score pondéré comme métrique\n",
        "    cv=3,                   # Validation croisée à 3 plis\n",
        "    verbose=3,              # Afficher les logs\n",
        "    n_jobs=-1               # Utiliser tous les cœurs disponibles\n",
        ")\n",
        "\n",
        "# Lancer l'optimisation\n",
        "grid_search.fit(data_preprocessed['cleaned_text'], y_train)\n",
        "\n",
        "# Afficher les meilleurs paramètres et le score associé\n",
        "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
        "print(\"Meilleur score F1 pondéré :\", grid_search.best_score_)\n",
        "\n",
        "# Sauvegarder le modèle optimisé\n",
        "best_svm = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "QPIZfxu0bRX3",
        "outputId": "b8c37196-ec64-4cde-b402-a4b19c8dbe6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f3ddda3ee16f>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Lancer l'optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Afficher les meilleurs paramètres et le score associé\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimisation"
      ],
      "metadata": {
        "id": "vFFpdmr2ou6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger le fichier .pkl\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "\n",
        "# Définir la variable cible\n",
        "y = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Supprimer les colonnes inutiles (par exemple, identifiants ou colonnes textuelles longues)\n",
        "columns_to_drop = ['OGC_FID', 'texte_annonce']  # Ajuster selon vos données\n",
        "X = data_preprocessed.drop(columns=columns_to_drop + ['cal_réponse_signalement'], errors='ignore')\n",
        "\n",
        "# Identifier les colonnes qualitatives restantes et les encoder si nécessaire\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "for col in non_numeric_columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Séparer les données en ensemble d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Étape 1 : Réduction des variables avec RandomForestClassifier ---\n",
        "# Entraîner un modèle de forêt aléatoire pour évaluer l'importance des variables\n",
        "forest = RandomForestClassifier(random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "# Importance des variables\n",
        "importances = forest.feature_importances_\n",
        "importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
        "print(\"\\nTop 10 variables importantes :\\n\", importance_df.head(10))\n",
        "\n",
        "# Réduire les variables en ne gardant que celles au-dessus d'un seuil d'importance\n",
        "threshold = 0.01\n",
        "important_features = importance_df[importance_df['Importance'] > threshold]['Feature'].tolist()\n",
        "X_train_reduced = X_train[important_features]\n",
        "X_test_reduced = X_test[important_features]\n",
        "\n",
        "# Vérifier les dimensions après réduction\n",
        "print(f\"\\nNombre de colonnes après réduction : {X_train_reduced.shape[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU6FoYS1hGlc",
        "outputId": "296deaa1-7a7a-4be0-8aea-51dadaafb04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 variables importantes :\n",
            "                             Feature  Importance\n",
            "11                   boamp_intitule    0.065873\n",
            "2                 boamp_theme_boamp    0.057482\n",
            "7                boamp_nom_acheteur    0.054459\n",
            "22           cal_insee_commune_exec    0.054423\n",
            "25                     cleaned_text    0.052405\n",
            "16                   cal_mot_rang_1    0.052240\n",
            "17                   cal_mot_rang_2    0.051991\n",
            "6   boamp_num_departement_diffusion    0.051126\n",
            "26                      text_length    0.050604\n",
            "23             cal_nom_commune_exec    0.050174\n",
            "\n",
            "Nombre de colonnes après réduction : 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Réentraîner le modèle avec les données originales et gestion des poids\n",
        "forest_weighted = RandomForestClassifier(\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=1,\n",
        "    min_samples_split=2,\n",
        "    n_estimators=50,\n",
        "    class_weight='balanced',  # Gestion automatique des poids\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entraîner le modèle\n",
        "forest_weighted.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Faire des prédictions sur les données de test\n",
        "y_pred_weighted = forest_weighted.predict(X_test_reduced)\n",
        "\n",
        "# Évaluer les performances\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "report_weighted = classification_report(y_test, y_pred_weighted)\n",
        "\n",
        "print(\"\\n--- Résultats avec Forêt Aléatoire et Gestion des Poids ---\")\n",
        "print(\"Accuracy :\", accuracy_weighted)\n",
        "print(\"\\nClassification Report:\\n\", report_weighted)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF4IT3KdR9HO",
        "outputId": "f2def932-f494-4fb1-fdd4-e654adddbaca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Résultats avec Forêt Aléatoire et Gestion des Poids ---\n",
            "Accuracy : 0.753\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     Pris en compte       0.76      0.98      0.85       741\n",
            "Rejete (hors specs)       0.63      0.11      0.19       259\n",
            "\n",
            "           accuracy                           0.75      1000\n",
            "          macro avg       0.69      0.54      0.52      1000\n",
            "       weighted avg       0.73      0.75      0.68      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Charger les données d'entraînement prétraitées\n",
        "data_preprocessed = pd.read_pickle(\"data_preprocessed.pkl\")\n",
        "y_train = data_preprocessed['cal_réponse_signalement']\n",
        "\n",
        "# Vectoriser les données d'entraînement\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(data_preprocessed['cleaned_text'])\n",
        "\n",
        "# Charger et prétraiter les données de test\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].str.lower().str.replace(r'[^a-z\\s]', '', regex=True)\n",
        "\n",
        "# Transformer les données de test avec le même vectoriseur TF-IDF\n",
        "X_test = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# Initialiser et entraîner le modèle de régression logistique\n",
        "log_reg = LogisticRegression(C=10, penalty='l2', solver='saga', max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les valeurs pour cal_réponse_signalement\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Ajouter les prédictions dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement'] = y_pred\n",
        "\n",
        "# Créer le fichier final avec uniquement les colonnes OCG_FID et cal_réponse_signalement\n",
        "final_output = test_data[['OGC_FID', 'cal_réponse_signalement']]\n",
        "\n",
        "# Exporter le fichier final en CSV\n",
        "final_output.to_csv(\"final_test_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Le fichier 'final_test_predictions.csv' a été généré avec succès.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGg5J2kMAsR8",
        "outputId": "d9d55606-f3b5-43a3-f657-1f4b078e1a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier 'final_test_predictions.csv' a été généré avec succès.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger les données de test avec la vérité terrain\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "# Charger les prédictions finales\n",
        "test_predictions = pd.read_csv(\"/content/final_test_predictions.csv\")\n",
        "# Associer les prédictions aux vérités terrain en utilisant la colonne `OGC_FID`\n",
        "merged_data = test_data.merge(test_predictions, on='OGC_FID', suffixes=('_réelle', '_prédite'))\n",
        "\n",
        "# Vérifier si les colonnes nécessaires existent\n",
        "if 'cal_réponse_signalement' in merged_data.columns and 'cal_réponse_signalement_prédite' in merged_data.columns:\n",
        "    # Calculer l'accuracy\n",
        "    accuracy = accuracy_score(merged_data['cal_réponse_signalement'], merged_data['cal_réponse_signalement_prédite'])\n",
        "\n",
        "    # Générer le rapport de classification\n",
        "    report = classification_report(merged_data['cal_réponse_signalement'], merged_data['cal_réponse_signalement_prédite'])\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(merged_data['cal_réponse_signalement'], merged_data['cal_réponse_signalement_prédite'])\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Rejeté\", \"Pris en compte\"])\n",
        "\n",
        "    print(f\"Accuracy : {accuracy:.3f}\\n\")\n",
        "    print(\"Rapport de classification :\\n\", report)\n",
        "\n",
        "    # Afficher la matrice de confusion\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(\"Matrice de Confusion : Modèle Final\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Les colonnes nécessaires (prédictions ou vérités terrain) ne sont pas disponibles.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K529LvWPDn68",
        "outputId": "cea376ad-8585-4b46-9d3c-06647a127262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les colonnes nécessaires (prédictions ou vérités terrain) ne sont pas disponibles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "important_features = [\n",
        "    'boamp_intitule', 'boamp_theme_boamp', 'boamp_nom_acheteur',\n",
        "    'cal_insee_commune_exec', 'cleaned_text', 'cal_mot_rang_1',\n",
        "    'cal_mot_rang_2', 'boamp_num_departement_diffusion', 'text_length',\n",
        "    'cal_nom_commune_exec'\n",
        "]\n",
        "# Les colonnes utilisées dans votre DataFrame d'entraînement\n",
        "top_29_features = X_train.columns.tolist()\n",
        "\n",
        "# Vérifiez le contenu de la liste\n",
        "print(\"Liste des 29 features utilisées dans le modèle :\")\n",
        "print(top_29_features)\n",
        "print(f\"Nombre de features : {len(top_29_features)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiC000rMxIii",
        "outputId": "a47cb6c9-7859-4a09-cdab-22232bfa4d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste des 29 features utilisées dans le modèle :\n",
            "['boamp_id_annonce', 'boamp_parent_annonce', 'boamp_theme_boamp', 'boamp_libelle_annonce', 'boamp_statut_annonce', 'boamp_date_fin_de_marche', 'boamp_num_departement_diffusion', 'boamp_nom_acheteur', 'boamp_siret_acheteur', 'boamp_type_organisme', 'boamp_lieu_exec', 'boamp_intitule', 'cal_fichier_pdf', 'cal_num_signalement', 'cal_theme_signalement', 'cal_statut_workflow', 'cal_mot_rang_1', 'cal_mot_rang_2', 'cal_date_traitement', 'cal_type_localisation', 'cal_num_departement_exec', 'cal_siren_epci', 'cal_insee_commune_exec', 'cal_nom_commune_exec', 'cal_adresse_annonce', 'cleaned_text', 'text_length', 'contains_url', 'contains_numbers']\n",
            "Nombre de features : 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 1 : Charger les données de test\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "\n",
        "# Étape 2 : Nettoyage et lemmatisation du texte\n",
        "test_data['cleaned_text'] = test_data['texte_annonce'].apply(clean_text).apply(lemmatize_text)\n",
        "\n",
        "# Étape 3 : Vectorisation avec le TF-IDF déjà entraîné\n",
        "# Important : Utiliser le vectoriseur TF-IDF ajusté sur les données d'entraînement\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# Étape 4 : Réduction de dimension (SVD)\n",
        "# Important : Utiliser le modèle SVD déjà entraîné\n",
        "X_test_reduced = svd.transform(X_test_tfidf)\n",
        "\n",
        "# Étape 5 : Création des variables additionnelles\n",
        "test_data['text_length'] = test_data['cleaned_text'].apply(lambda x: len(x.split()))\n",
        "test_data['contains_url'] = test_data['texte_annonce'].apply(lambda x: 1 if 'http' in x else 0)\n",
        "test_data['contains_numbers'] = test_data['texte_annonce'].apply(lambda x: 1 if re.search(r'\\d', x) else 0)\n",
        "\n",
        "# Étape 6 : Sélection des colonnes additionnelles et conversion en array\n",
        "additional_features = test_data[['text_length', 'contains_url', 'contains_numbers']].values\n",
        "\n",
        "# Étape 7 : Combinaison des variables réduites et additionnelles\n",
        "X_test_combined = np.hstack((X_test_reduced, additional_features))\n",
        "\n",
        "# Étape 8 : Sélection des 29 features importants\n",
        "# Important : Utiliser les mêmes features que ceux sélectionnés pour l'entraînement\n",
        "selected_features = top_29_features\n",
        "X_test_final = test_data[selected_features].values\n",
        "\n"
      ],
      "metadata": {
        "id": "9efUdrEAyi1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification des dimensions\n",
        "if X_test_final.shape[1] != X_train.shape[1]:\n",
        "    raise ValueError(f\"Incompatibilité : X_test_final a {X_test_final.shape[1]} features, mais le modèle en attend {X_train.shape[1]}.\")\n"
      ],
      "metadata": {
        "id": "EEXR6nnA4FnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_features = [col for col in important_features if col not in test_data.columns]\n",
        "if missing_features:\n",
        "    print(f\"Colonnes manquantes dans les données de test : {missing_features}\")\n",
        "else:\n",
        "    print(\"Toutes les colonnes importantes sont disponibles.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKcDh4Hl5vDg",
        "outputId": "510a0db2-4782-47ed-e7b3-c745cc77333f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toutes les colonnes importantes sont disponibles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 9 : Faire des prédictions avec le modèle optimisé\n",
        "y_test_pred = best_forest.predict(X_test_final)\n",
        "y_test_probs = best_forest.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "# Étape 10 : Ajouter les prédictions et scores dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement_prédite'] = y_test_pred\n",
        "test_data['confidence'] = y_test_probs\n",
        "\n",
        "# Étape 11 : Exporter les prédictions pour analyse\n",
        "test_data[['OGC_FID', 'cal_réponse_signalement_prédite']].to_csv(\"test_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Les prédictions ont été exportées dans 'test_predictions.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iU1crmeE3OKM",
        "outputId": "4283a91e-0c76-4e78-c14e-216283d4e8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_forest' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4f18ea4516fc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Étape 9 : Faire des prédictions avec le modèle optimisé\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Étape 10 : Ajouter les prédictions et scores dans le DataFrame de test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_forest' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if X_test_final.shape[1] != X_train.shape[1]:\n",
        "    print(f\"Incompatibilité : X_test_final a {X_test_final.shape[1]} features, mais le modèle en attend {X_train.shape[1]}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSI37nEH6bYe",
        "outputId": "5518601a-aad5-4fa1-f809-d8a382824343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incompatibilité : X_test_final a 103 features, mais le modèle en attend 29.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming 'train_data' is your training data DataFrame\n",
        "# ... (your previous code to load and preprocess train_data) ...\n",
        "\n",
        "# 1. Re-fit TF-IDF Vectorizer on the combined text data of train and test sets\n",
        "all_text_data = pd.concat([train_data['cleaned_text'], test_data['cleaned_text']])\n",
        "tfidf_vectorizer = TfidfVectorizer()  # Or use your previous parameters\n",
        "tfidf_vectorizer.fit(all_text_data)\n",
        "\n",
        "# 2. Transform train and test data with the re-fitted TF-IDF Vectorizer\n",
        "X_train_tfidf = tfidf_vectorizer.transform(train_data['cleaned_text'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['cleaned_text'])\n",
        "\n",
        "# 3. Re-fit SVD on the transformed training data\n",
        "svd.fit(X_train_tfidf)  # Or use your previous parameters\n",
        "\n",
        "# 4. Transform both train and test data with the re-fitted SVD\n",
        "X_train_reduced = svd.transform(X_train_tfidf)\n",
        "X_test_reduced = svd.transform(X_test_tfidf)\n",
        "\n",
        "# 5. Continue with creating and combining additional features as before\n",
        "# ... (your existing code for additional features and combining them) ...\n",
        "\n",
        "# 6. Retrain your model (best_forest) with the new transformed training data\n",
        "# ... (your existing code for model training) ...\n",
        "\n",
        "# 7. Now predict using the updated test data\n",
        "y_test_pred = best_forest.predict(X_test_final)\n",
        "# ... (rest of your prediction code) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ocvBeRZc6Q2B",
        "outputId": "6e20f60c-0c09-4db8-8e49-09b8b78a28ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-2c78e8ca0b18>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. Re-fit TF-IDF Vectorizer on the combined text data of train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mall_text_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Or use your previous parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# Étape 9 : Analyse des résultats (si étiquettes disponibles dans test_data)\n",
        "if 'cal_réponse_signalement' in test_data.columns:\n",
        "    print(\"\\n--- Analyse des Résultats ---\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(test_data['cal_réponse_signalement'], y_test_pred))\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(test_data['cal_réponse_signalement'], y_test_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Rejeté\", \"Pris en compte\"])\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(\"Matrice de Confusion : Forêt Aléatoire Optimisée\")\n",
        "    plt.show()\n",
        "\n",
        "    # Afficher l'Accuracy\n",
        "    accuracy = accuracy_score(test_data['cal_réponse_signalement'], y_test_pred)\n",
        "    print(f\"\\nAccuracy sur les données de test : {accuracy:.3f}\")\n",
        "\n",
        "# Étape 10 : Exporter les prédictions pour analyse\n",
        "test_data[['OGC_FID', 'cal_réponse_signalement_prédite', 'confidence']].to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"\\nLes prédictions ont été exportées dans 'test_predictions.csv'.\")\n"
      ],
      "metadata": {
        "id": "FCqcZaRN0yyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faire des prédictions avec le modèle optimisé\n",
        "y_test_pred = best_forest.predict(X_test_final)\n",
        "y_test_probs = best_forest.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "# Ajouter les prédictions et scores dans le DataFrame de test\n",
        "test_data['cal_réponse_signalement_prédite'] = y_test_pred\n",
        "test_data['confidence'] = y_test_probs\n",
        "\n",
        "# Analyse des résultats\n",
        "print(\"\\n--- Analyse des Résultats ---\")\n",
        "if 'cal_réponse_signalement' in test_data.columns:  # Si les vraies étiquettes sont disponibles\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(test_data['cal_réponse_signalement'], y_test_pred))\n",
        "\n",
        "    # Afficher la matrice de confusion\n",
        "    cm = confusion_matrix(test_data['cal_réponse_signalement'], y_test_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Rejeté\", \"Pris en compte\"])\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(\"Matrice de Confusion : Forêt Aléatoire Optimisée\")\n",
        "    plt.show()\n",
        "\n",
        "    # Afficher l'Accuracy\n",
        "    accuracy = accuracy_score(test_data['cal_réponse_signalement'], y_test_pred)\n",
        "    print(f\"\\nAccuracy sur les données de test : {accuracy:.3f}\")\n",
        "\n",
        "# Exporter les prédictions pour analyse\n",
        "test_data[['OGC_FID', 'cal_réponse_signalement_prédite', 'confidence']].to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"Les prédictions ont été exportées dans 'test_predictions.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "KKzoJpBoxZoD",
        "outputId": "e55ceb81-f1bc-4829-e23f-c7062ab4a57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: \"Travaux de création d'une piste cyclable située rue des Poissonniers/Cantelaude/Moulin à Le Teich\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-c30b823837cd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Faire des prédictions avec le modèle optimisé\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ajouter les prédictions et scores dans le DataFrame de test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \"\"\"\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mforce_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"Travaux de création d'une piste cyclable située rue des Poissonniers/Cantelaude/Moulin à Le Teich\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Charger les données de test\n",
        "test_data = pd.read_json(\"test.jsonl\", lines=True)\n",
        "\n",
        "# Réduction des variables sur les données de test\n",
        "X_test_reduced = test_data[important_features]  # Utilisez les variables importantes identifiées\n",
        "\n",
        "# Prédire les classes sur les données de test avec le modèle pondéré\n",
        "y_test_pred_weighted = forest_weighted.predict(X_test_reduced)\n",
        "\n",
        "# Ajouter les prédictions dans le DataFrame des données de test\n",
        "test_data['cal_réponse_signalement_prédite'] = y_test_pred_weighted\n",
        "\n",
        "# Exporter les prédictions\n",
        "test_data[['OGC_FID', 'cal_réponse_signalement_prédite']].to_csv(\"test_predictions_weighted.csv\", index=False)\n",
        "\n",
        "print(\"\\nLes prédictions ont été enregistrées dans 'test_predictions_weighted.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "n5HswjExwWOq",
        "outputId": "28bf51b6-7d58-44d2-d287-7176a72a9020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['cleaned_text', 'text_length'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-f09bd9b905e1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Réduction des variables sur les données de test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_test_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportant_features\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Utilisez les variables importantes identifiées\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Prédire les classes sur les données de test avec le modèle pondéré\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['cleaned_text', 'text_length'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxnH09V665Ab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}